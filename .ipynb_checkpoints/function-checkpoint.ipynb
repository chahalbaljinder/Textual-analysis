{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32308d39-1d59-4382-b50f-cb0c078a2a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize , sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55d2b777-e089-4a96-8a08-3f2967844c48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a185f3a9-b045-4ae5-a19e-c33b42ac4792",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_count(file_path):\n",
    "    \"\"\"\n",
    "    Reads text from a file, tokenizes it into words, converts them to lowercase, and returns the list of words\n",
    "    and their count.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str, the path to the text file\n",
    "\n",
    "    Returns:\n",
    "    - tuple: (list of tokenized words, int count of words)\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Tokenize by words and convert to lowercase\n",
    "    words = text.split()\n",
    "    words = [word.lower() for word in words]\n",
    "    \n",
    "    # Return the list of words and their count\n",
    "    return words, len(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb035d71-8132-40f9-b842-88616c3c18d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_tokenize(words):\n",
    "    # Initialize lists\n",
    "    cleaned_txt = []\n",
    "    \n",
    "    # Define stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Remove stop words\n",
    "    for i in words:\n",
    "        if i.lower() not in stop_words:\n",
    "            cleaned_txt.append(i)\n",
    "    \n",
    "    # Join list into a single string\n",
    "    cleaned_txt = \" \".join(cleaned_txt)\n",
    "    \n",
    "    # Remove digits and digits within brackets\n",
    "    cleaned_txt = re.sub(r'[0-9]', \"\", cleaned_txt)\n",
    "    cleaned_txt = re.sub(r'\\[\\d+\\]', \"\", cleaned_txt)\n",
    "    \n",
    "    # Tokenize words\n",
    "    word_token = word_tokenize(cleaned_txt)\n",
    "    \n",
    "    # Apply stemming\n",
    "    p = PorterStemmer()\n",
    "    porter_stemmer = [p.stem(word) for word in word_token]\n",
    "    \n",
    "    # Join stemmed words into a single string\n",
    "    cleaned_txt = \" \".join(porter_stemmer)\n",
    "    \n",
    "    # Tokenize sentences\n",
    "    sent_token = sent_tokenize(cleaned_txt)\n",
    "    \n",
    "    return sent_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edcfae90-1b92-40fa-b290-91e8b5f36062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def analyze_sentiments(sentences):\n",
    "    \"\"\"\n",
    "    Analyzes sentiment of a list of sentences and classifies each sentence as positive, neutral, or negative.\n",
    "\n",
    "    Parameters:\n",
    "    - sentences: list of str, list containing sentences to be analyzed\n",
    "\n",
    "    Returns:\n",
    "    - tuple: (list of sentences, list of polarity scores, list of sentiment labels)\n",
    "    \"\"\"\n",
    "    sentence_list = []\n",
    "    polarity_act = []\n",
    "    polarity_list = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        blob = TextBlob(sentence)  # Create a TextBlob object\n",
    "        polarity = blob.sentiment.polarity  # Get the polarity score\n",
    "        sentence_list.append(sentence)\n",
    "        polarity_act.append(polarity)\n",
    "\n",
    "        # Determine sentiment and append to polarity_list\n",
    "        if polarity > 0:\n",
    "            sentiment_label = 1  # Positive\n",
    "            print(f\"Sentence: {sentence}\")\n",
    "            print(f\"Polarity: {polarity}\")\n",
    "            print(f\"Sentiment: Positive\")\n",
    "            polarity_list.append(sentiment_label)\n",
    "        elif polarity == 0:\n",
    "            sentiment_label = 0  # Neutral\n",
    "            print(f\"Sentence: {sentence}\")\n",
    "            print(f\"Polarity: {polarity}\")\n",
    "            print(f\"Sentiment: Neutral\")\n",
    "            polarity_list.append(sentiment_label)\n",
    "        else:\n",
    "            sentiment_label = -1  # Negative\n",
    "            print(f\"Sentence: {sentence}\")\n",
    "            print(f\"Polarity: {polarity}\")\n",
    "            print(f\"Sentiment: Negative\")\n",
    "            polarity_list.append(sentiment_label)\n",
    "        \n",
    "        print(\"\\t\")\n",
    "    \n",
    "    return sentence_list, polarity_act, polarity_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e34a8479-fe18-47cd-b922-88d57c51fa93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: life often present complex tapestri experi , fill triumph challeng .\n",
      "Polarity: -0.15\n",
      "Sentiment: Negative\n",
      "\t\n",
      "Sentence: one hand , sunni day joy moment testament beauti life offer .\n",
      "Polarity: 0.8\n",
      "Sentiment: Positive\n",
      "\t\n",
      "Sentence: instanc , receiv unexpect promot work bring immens satisfact sens achiev .\n",
      "Polarity: 0.0\n",
      "Sentiment: Neutral\n",
      "\t\n",
      "Sentence: similarli , enjoy peac walk park crisp autumn day provid refresh escap daili stress .\n",
      "Polarity: 0.325\n",
      "Sentiment: Positive\n",
      "\t\n",
      "Sentence: moment joy often surround support love one , make even memor .\n",
      "Polarity: 0.65\n",
      "Sentiment: Positive\n",
      "\t\n",
      "Sentence: celebr , birthday anniversari , occas reinforc bond share closest us .\n",
      "Polarity: 0.0\n",
      "Sentiment: Neutral\n",
      "\t\n",
      "Sentence: howev , life alway smooth sail .\n",
      "Polarity: 0.4\n",
      "Sentiment: Positive\n",
      "\t\n",
      "Sentence: sometim , encount setback feel overwhelm .\n",
      "Polarity: 0.0\n",
      "Sentiment: Neutral\n",
      "\t\n",
      "Sentence: exampl , sudden health issu unforeseen financi problem creat signific amount stress uncertainti .\n",
      "Polarity: 0.0\n",
      "Sentiment: Neutral\n",
      "\t\n",
      "Sentence: challeng overshadow posit aspect live , make difficult maintain hope outlook .\n",
      "Polarity: -0.18181818181818182\n",
      "Sentiment: Negative\n",
      "\t\n",
      "Sentence: feel failur project go plan dishearten , may take time regain confid motiv .\n",
      "Polarity: 0.0\n",
      "Sentiment: Neutral\n",
      "\t\n",
      "Sentence: despit difficulti , essenti rememb resili persever lead person growth .\n",
      "Polarity: 0.0\n",
      "Sentiment: Neutral\n",
      "\t\n",
      "Sentence: overcom advers often reveal inner strength provid new perspect life .\n",
      "Polarity: 0.06818181818181818\n",
      "Sentiment: Positive\n",
      "\t\n",
      "Sentence: face challeng head-on empow , lesson learn experi often contribut deeper understand oneself .\n",
      "Polarity: 0.0\n",
      "Sentiment: Neutral\n",
      "\t\n",
      "Sentence: success achiev overcom obstacl often reward , testament one 's determin hard work .\n",
      "Polarity: 0.004166666666666652\n",
      "Sentiment: Positive\n",
      "\t\n",
      "Sentence: posit side , moment joy frequent amplifi presenc support friend famili .\n",
      "Polarity: 0.45\n",
      "Sentiment: Positive\n",
      "\t\n",
      "Sentence: kind word colleagu thought gestur friend significantli uplift one 's mood provid much-need encourag .\n",
      "Polarity: 0.6\n",
      "Sentiment: Positive\n",
      "\t\n",
      "Sentence: small act kind remind us good world help counterbal neg experi face .\n",
      "Polarity: 0.3499999999999999\n",
      "Sentiment: Positive\n",
      "\t\n",
      "Sentence: howev , relationship free conflict .\n",
      "Polarity: 0.4\n",
      "Sentiment: Positive\n",
      "\t\n",
      "Sentence: disagr misunderstand strain even closest bond .\n",
      "Polarity: 0.0\n",
      "Sentiment: Neutral\n",
      "\t\n",
      "Sentence: navig issu requir patienc effect commun , sometim , resolut may take longer anticip .\n",
      "Polarity: 0.0\n",
      "Sentiment: Neutral\n",
      "\t\n",
      "Sentence: emot toll conflict signific , work often lead stronger , resili relationship .\n",
      "Polarity: 0.0\n",
      "Sentiment: Neutral\n",
      "\t\n",
      "Sentence: workplac , achiev recognit gratifi , also time job dissatisfact workplac dynam pose signific challeng .\n",
      "Polarity: 0.0\n",
      "Sentiment: Neutral\n",
      "\t\n",
      "Sentence: handl difficult colleagu deal high-pressur environ stress , find balanc profession respons person well-b becom crucial .\n",
      "Polarity: -0.25\n",
      "Sentiment: Negative\n",
      "\t\n",
      "Sentence: ultim , life blend posit neg experi , contribut overal growth understand .\n",
      "Polarity: 0.0\n",
      "Sentiment: Neutral\n",
      "\t\n",
      "Sentence: embrac high low allow us appreci full spectrum experi .\n",
      "Polarity: 0.17\n",
      "Sentiment: Positive\n",
      "\t\n",
      "Sentence: celebr victori , matter small , learn setback integr part journey .\n",
      "Polarity: -0.25\n",
      "Sentiment: Negative\n",
      "\t\n",
      "Sentence: abil find joy everyday moment , even face challeng , help cultiv balanc fulfil life .\n",
      "Polarity: 0.30000000000000004\n",
      "Sentiment: Positive\n",
      "\t\n"
     ]
    }
   ],
   "source": [
    "file_path = 'test_sample.txt'\n",
    "words, count = tokenize_and_count(file_path)\n",
    "sent_tokens = clean_and_tokenize(words)\n",
    "sentence_list, polarity_act, polarity_list = analyze_sentiments(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "757523c8-50ec-42a8-9af0-1a05348bf7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = pd.DataFrame({\n",
    "    \"Sentence\" : sentence_list, \n",
    "    \"Polarity_val\" : polarity_list,\n",
    "    \"Polarity_score\" : polarity_act\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8304e051-689e-462a-b962-fd8c962b3e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample.to_csv(\"test_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcfb6ac-767e-4645-a663-a1b1f764cb35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
