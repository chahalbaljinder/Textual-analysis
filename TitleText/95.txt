NFT Data Automation (looksrare), and ETL tool
Healthcare AI ChatBot using LLAMA, LLM, LangchainEfficient Supply Chain Assessment: Overcoming Technical Hurdles for Web Application DevelopmentStreamlined Integration: Interactive Brokers API with Python for Desktop Trading ApplicationEfficient Data Integration and User-Friendly Interface Development: Navigating Challenges in Web Application DeploymentAI Chatbot using LLM, Langchain, LLamaAI Bot Audio to audioMethodology for ETL Discovery Tool using LLMA, OpenAI, LangchainMethodology for database discovery tool using openai, LLMA, LangchainRising IT cities and its impact on the economy, environment, infrastructure, and city life by the year 2040.Rising IT Cities and Their Impact on the Economy, Environment, Infrastructure, and City Life in FutureInternet Demand’s Evolution, Communication Impact, and 2035’s Alternative PathwaysRise of Cybercrime and its Effect in upcoming FutureAI/ML and Predictive ModelingSolution for Contact Centre ProblemsHow to Setup Custom Domain for Google App Engine Application?Code Review ChecklistClient: A leading tech firm in the USAIndustry Type: IT ServicesServices: Blockchain, NFTOrganization Size: 10+To scrape all the desired information regarding the NFTs from a website and store them in a database to be accessed later on.Matthew Brown – extract all events, all time from this https://looksrare.org/explore/activity . We can then pay you weekly to keep them up to date. You can choose any technology you like, as long as it’s updated into an SQL database. Additional tasks may be to make an alert or dashboard from data, later access API when it becomes available.We provided a robust solution which returned the NFT data every 8 hours into the google big query database. To do this we used selenium web driver to scrape all events as the website was dynamic and did not have a format data structure to scrape data using AJAX POST calls. After automating the scarper the data was manipulated and constructed into a desired format into pandas dataframe, which was later used to push the dataframe into the google big query database using Google cloud api and credentials. The data was getting collected every day and about 50M distinct rows were created.SQL Google BigQueryGoogle BigQueryThe only technical challenge faced during this project was that the website used to keep changing the elements on their webpage and used to cause error. Though it did not use to happen regularly, it happened 3 times in 5 weeks. Also AJAX calls were not proper.Identifying the elements solved the issue. Also remote access to a better desktop enabled me to keep working as well as keep the code running all the time.We provide intelligence, accelerate innovation and implement technology with extraordinary breadth and depth global insights into the big data,data-driven dashboards, applications development, and information management for organizations through combining unique, specialist services and high-lvel human expertise.Contact us: hello@blackcoffer.com© All Right Reserved, Blackcoffer(OPC) Pvt. Ltd